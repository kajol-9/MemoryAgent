{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9362759-d0c9-4c48-b061-dd14d1356283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing self-editing memory from scratch\n",
    "\n",
    "This module demonstrates how to build long-term memory capabilities in LLM agents using a technique called self-editing memory, inspired by the MemGPT paper.\n",
    "\n",
    "Traditionally, methods like RAG (Retrieval-Augmented Generation) or recursive summarization are used to maintain memory in LLMs. However, MemGPT introduces a powerful alternative: memory management to the LLM itself ‚Äî treating it as the most intelligent component of the system.\n",
    "\n",
    "üí° Instead of hard-coding memory updates or writing manual memory pruning logic, we use OpenAI's tool-calling to let the LLM autonomously decide:\n",
    "\n",
    "What to remember\n",
    "\n",
    "When to update memory\n",
    "\n",
    "How to format and store memory internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5d0ec-3901-4ac4-b961-68c789ad8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb66277-d6ce-4dcb-ae35-d40a1b9b4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50fbde-fa50-43ce-8758-d9d1b7ead458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12285574-783f-4b16-92bd-6450d2f5943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa60d4-3429-4efa-a891-5be07c4640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=get_openai_api_key())\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "#to confirm connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1d592-45ca-41e1-9a35-d367cdd649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Breaking down the LLM context window\n",
    "\n",
    "\n",
    "### Context Window Design in MemoryAgent\n",
    "In LLM-based agents, context is everything. Since large language models have a limited context window (a fixed number of tokens the model can \"see\" at once), it's essential to manage that space intelligently.\n",
    "\n",
    "This project implements a structured context window for the agent, drawing on architectural insights from the MemGPT framework.\n",
    "\n",
    "###  Agent Context Layout\n",
    "MemoryAgent structures its prompt using the following key components:\n",
    "\n",
    "System Prompt: Defines the personality and behavior rules of the agent.\n",
    "\n",
    "Conversation History: Recent messages exchanged between the user and the assistant.\n",
    "\n",
    "[MEMORY] Section: A custom-defined core memory area that contains structured knowledge (e.g., facts about the user, prior events, goals). This section is read-writeable by the agent via tool-calling.\n",
    "\n",
    "Recursive Summary (Optional): A synthesized history summary that compresses old interactions, useful when message history exceeds the token limit.\n",
    "\n",
    "###  Why This Matters\n",
    "By explicitly dividing the context window, MemoryAgent can:\n",
    "\n",
    "Maintain a consistent personality and role\n",
    "\n",
    "Recall essential facts even across long conversations\n",
    "\n",
    "Reduce token usage through smart summarization\n",
    "\n",
    "Store persistent knowledge in the core memory, while keeping the conversation responsive\n",
    "\n",
    "This structured design is crucial for building scalable, intelligent agents that don‚Äôt ‚Äúforget‚Äù over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd4fcd-6356-4596-a9cf-4c45deeb55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A simple agent's context window\n",
    "In the code below, you can see how we can define an agent with a system prompt. The system prompt will be included in every chat completions request as the first message, while later messages will change over time as the user and assistant exchange messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f9f43-3f66-43d1-b57a-25b2577ed74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d024907-0774-4aa7-ac20-9a1f2ccd1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a chatbot.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc69db1-f38c-4bfc-b863-e466ad137d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the completion request with the tool usage\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        # system prompt: always included in the context window \n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        # chat history (evolves over time)\n",
    "        {\"role\": \"user\", \"content\": \"What is my name?\"}, \n",
    "    ]\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11c1ff-e1c8-4a27-aa86-03473110b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding memory to the context \n",
    "Similar to how we always start a chat completitions request with a system prompt, we can also prefix the list of messages with a prompt containing important memories. Lets see how we can add a memory section to the context window, and have the agent use that memory to respond to user messages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c08bf-8697-444d-8841-4e0860bbdcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_memory = {\"human\": \"Name: Bob\"}\n",
    "system_prompt = \"You are a chatbot. \" \\\n",
    "+ \"You have a section of your context called [MEMORY] \" \\\n",
    "+ \"that contains information relevant to your conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd24a8-ff3b-4986-8d61-9c61a64f0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        # system prompt \n",
    "        {\"role\": \"system\", \"content\": system_prompt + \"[MEMORY]\\n\" + json.dumps(agent_memory)},\n",
    "        # chat history \n",
    "        {\"role\": \"user\", \"content\": \"What is my name?\"},\n",
    "    ],\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c85e7d-4500-44a6-9314-7c18001c1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we will  make this memory read-writeable by the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64a017-fda3-47eb-ae35-368485cd9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Modifing the memory with tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790701c-09d6-45bd-ab48-49251aa171c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "We first need to define a memory save tool. In order to allow the agent to save new memory, we implement a simple tool that appends to a section of the memory dictionary which we will pass to the agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067ea8f-0da7-49a3-be29-c173df52b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining a memory editing tool \n",
    "Instead of directly providing the name in the agent's memory, we'll instead start with a blank memory object and provide a function that can edit the memory object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d12bb4-f67f-4979-a8fd-962693a68c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_memory = {\"human\": \"\", \"agent\": \"\"}\n",
    "\n",
    "def core_memory_save(section: str, memory: str): \n",
    "    agent_memory[section] += '\\n' \n",
    "    agent_memory[section] += memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bc944-493e-4cd8-a981-8e496b544532",
   "metadata": {},
   "outputs": [],
   "source": [
    "To inform our agent of this tool and how to use it, we need to create a tool schema that OpenAI can process. This includes a description of how to use the tool, and the parameters the agent must generate to input into the tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55222ed-1d4f-403a-8295-31709324a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tool description \n",
    "core_memory_save_description = \"Save important information about you,\" \\\n",
    "+ \"the agent or the human you are chatting with.\"\n",
    "\n",
    "# arguments into the tool (generated by the LLM)\n",
    "# defines what the agent must generate to input into the tool \n",
    "core_memory_save_properties = \\\n",
    "{\n",
    "    # arg 1: section of memory to edit\n",
    "    \"section\": {\n",
    "        \"type\": \"string\",\n",
    "        \"enum\": [\"human\", \"agent\"],\n",
    "        \"description\": \"Must be either 'human' \" \\\n",
    "        + \"(to save information about the human) or 'agent'\" \\\n",
    "        + \"(to save information about yourself)\",            \n",
    "    },\n",
    "    # arg 2: memory to save\n",
    "    \"memory\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Memory to save in the section\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# tool schema (passed to OpenAI)\n",
    "core_memory_save_metadata = \\\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"core_memory_save\",\n",
    "            \"description\": core_memory_save_description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": core_memory_save_properties,\n",
    "                \"required\": [\"section\", \"memory\"],\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b60b57-edf2-4938-9c14-f68684d6d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we can pass the tool call into the agent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb445c9-07f9-48c1-8a88-6bccc05f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "agent_memory = {\"human\": \"\"}\n",
    "system_prompt = \"You are a chatbot. \" \\\n",
    "+ \"You have a section of your context called [MEMORY] \" \\\n",
    "+ \"that contains information relevant to your conversation\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        # system prompt \n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        # memory \n",
    "        {\"role\": \"system\", \"content\": \"[MEMORY]\\n\" + json.dumps(agent_memory)},\n",
    "        # chat history \n",
    "        {\"role\": \"user\", \"content\": \"My name is Bob\"},\n",
    "    ],\n",
    "    # tool schemas \n",
    "    tools=[core_memory_save_metadata]\n",
    ")\n",
    "response = chat_completion.choices[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bf186-90b6-4c46-ab09-fdb1f895a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Executing the tool \n",
    "Unfortunately, OpenAI isn't going to actually execute the tool, so we have to do this ourselves. Lets take the arguments specified in the tool call response we just got to run the tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d602454-e89c-407f-ac66-4fd75032e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = json.loads(response.message.tool_calls[0].function.arguments)\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd12d3b-ddf6-4d1f-845d-2b97ab10ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function with the specified arguments \n",
    "core_memory_save(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0e31b-f482-423e-b715-62e87e2ce0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we can view the memory object that has been updated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cee3fd-d600-4bdc-af4f-99ff426d3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bfeae-dff3-4a0c-87dc-18fa813a28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running the next agent step \n",
    "Now, we can see how the agent responds differently as the memory has been updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017c82e-cf16-49fb-9474-ca9ad3cb64f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        # system prompt \n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        # memory \n",
    "        {\"role\": \"system\", \"content\": \"[MEMORY]\\n\" + json.dumps(agent_memory)},\n",
    "        # chat history \n",
    "        {\"role\": \"user\", \"content\": \"what is my name\"},\n",
    "    ],\n",
    "    tools=[core_memory_save_metadata]\n",
    ")\n",
    "response = chat_completion.choices[0]\n",
    "response.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8f757-da16-4cc0-8f5f-46599dc4b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing an agentic loop\n",
    "In our current implementation, the agent can only take one step at a time: it can either edit memory, or respond to the user. However, ideally we want our agent to support *multi-step reasoning*, so it can combine multiple actions together. For example, if we tell the agent our name is \"Bob\", we want the agent to both edit its memory and also return back a message to us. \n",
    "\n",
    "We can implement multi-step reasoning by calling the `client.chat.completions.create(...)` in a loop, and allowing the agent to choose whether to continue its reasoning steps or to break out of the loop. For simplicity, we will assume that an agent response that is *not* a tool call breaks out of the reasoning loop yields control back to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abef740-a1e2-4bb6-9e15-6ee1438345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_memory = {\"human\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a7d1f-3950-4806-8485-feaa287cad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_os = system_prompt \\\n",
    "+ \"\\n. You must either call a tool (core_memory_save) or\" \\\n",
    "+ \"write a response to the user. \" \\\n",
    "+ \"Do not take the same actions multiple times!\" \\\n",
    "+ \"When you learn new information, make sure to always\" \\\n",
    "+ \"call the core_memory_save tool.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4c0b0-7b0e-46d3-b6d5-e29a5eff7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "We implement a simple step function for the agent. The function responds to the user message but allows the agent to take multiple actions in sequence, and returns to the user when a message (that does not call a tool) is sent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c4384-19d5-4d0e-b02d-bcdc9e524c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_step(user_message, chat_history = []): \n",
    "\n",
    "    # prefix messages with system prompt and memory\n",
    "    messages = [\n",
    "        # system prompt \n",
    "        {\"role\": \"system\", \"content\": system_prompt_os}, \n",
    "        # memory\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"[MEMORY]\\n\" + json.dumps(agent_memory)\n",
    "        },\n",
    "    ] \n",
    "\n",
    "    # append the chat history \n",
    "    messages += chat_history\n",
    "    \n",
    "\n",
    "    # append the most recent message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # agentic loop \n",
    "    while True: \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=[core_memory_save_metadata]\n",
    "        )\n",
    "        response = chat_completion.choices[0]\n",
    "\n",
    "        # update the messages with the agent's response \n",
    "        messages.append(response.message)\n",
    "\n",
    "        # if NOT calling a tool (responding to the user), return \n",
    "        if not response.message.tool_calls: \n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": response.message.content\n",
    "            })\n",
    "            return response.message.content\n",
    "\n",
    "        # if calling a tool, execute the tool\n",
    "        if response.message.tool_calls: \n",
    "            print(\"TOOL CALL:\", response.message.tool_calls[0].function)\n",
    "\n",
    "            # add the tool call response to the message history \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": response.message.tool_calls[0].id, \"name\": \"core_memory_save\", \"content\": f\"Updated memory: {json.dumps(agent_memory)}\"})\n",
    "\n",
    "            # parse the arguments from the LLM function call\n",
    "            arguments = json.loads(response.message.tool_calls[0].function.arguments)\n",
    "\n",
    "            # run the function with the specified arguments\n",
    "            core_memory_save(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474b7df-37e8-4841-ae2d-84dc4d4d308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we can observe the agent take multiple actions when we sent it a message: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498df79-aee6-4b72-ad2d-db1a11afc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_step(\"my name is bob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e790d-ca0e-425d-8def-0499d07f7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, the agent is able to both edit its memory *and* generate a response to the user that uses the updated memory in a single step. \n",
    "\n",
    "Although in this example, we only support a single tools and responding to the user, this same structure can be used to implement more complex reasoning loops that combine many tools. In MemGPT, all actions (even a response to the user) is a tool, where some tools (such as sending a message) break the agent reasoning loop, while other tools (such as searching the archival memory store or editing memory) do not. \n",
    "\n",
    "we have included self-editing memory and multi-step reasoning :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3cedf-d3e3-4d0e-90bb-5e55cf5bb5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e19c1-4502-4548-a0b6-f7683deb9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Building Agents with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca9641-25fc-4941-aa56-31943e0da7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install letta openai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d415f0d-6e86-4e1f-9c3b-2b173ca0717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup a Letta client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e076a53-6132-490d-a7ea-277e6bb173d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letta_token():\n",
    "    load_env()\n",
    "    return os.getenv(\"LETTA_API_TOKEN\")\n",
    "\n",
    "def get_letta_base_url():\n",
    "    load_env()\n",
    "    return os.getenv(\"LETTA_BASE_URL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36afa0c-7c8b-4045-a3c5-40d3461983bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta_client import Letta\n",
    "from helper import get_letta_token, get_letta_base_url\n",
    "\n",
    "client = Letta(\n",
    "    token=get_letta_token(),\n",
    "    base_url=get_letta_base_url()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fed9a-74d8-462e-a9da-08cafd908d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test it\n",
    "client.agents.list()  # Should return empty or list of agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531ea2b-9576-47a6-ace5-e93c8d14ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_message(message):\n",
    "    if message.message_type == \"reasoning_message\":\n",
    "        print(\"üß† Reasoning: \" + message.reasoning)\n",
    "    elif message.message_type == \"assistant_message\":\n",
    "        print(\"ü§ñ Agent: \" + message.content)\n",
    "    elif message.message_type == \"tool_call_message\":\n",
    "        print(\"üîß Tool Call: \" + message.tool_call.name + \"\\n\" + message.tool_call.arguments)\n",
    "    elif message.message_type == \"tool_return_message\":\n",
    "        print(\"üîß Tool Return: \" + message.tool_return)\n",
    "    elif message.message_type == \"user_message\":\n",
    "        print(\"üë§ User Message: \" + message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34437852-efbd-4ce1-9370-d3eb75e6495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a simple agent with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3e82f-1254-49d9-9d1b-6f795bf17da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d894c0b-3b18-493f-b23a-857ca880a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = client.agents.create(\n",
    "    name=\"simple_agent\",\n",
    "    memory_blocks=[\n",
    "        {\n",
    "          \"label\": \"human\",\n",
    "          \"value\": \"My name is Charles\",\n",
    "          \"limit\": 10000 # character limit\n",
    "        },\n",
    "        {\n",
    "          \"label\": \"persona\",\n",
    "          \"value\": \"You are a helpful assistant and you always use emojis\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    embedding=\"openai/text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfef81-6b1f-4883-a77f-1e3d0820d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Messaging an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdd8a6-f1d8-481f-935b-7d6590dc6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a message to the agent\n",
    "response = client.agents.messages.create(\n",
    "    agent_id=agent_state.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hows it going????\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# if we want to print the messages\n",
    "for message in response.messages:\n",
    "    print_message(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65416f-aca1-4076-93de-74952a4c926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viewing usage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4a380-7612-4ff0-a46a-f9fdfd78021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to print the usage stats\n",
    "print(response.usage.completion_tokens)\n",
    "print(response.usage.prompt_tokens)\n",
    "print(response.usage.step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499531a-419f-4974-9c6b-7a43b6894595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe58a8f-51c5-4c1f-aecc-fc34b0ca1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_state.system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676ffb0-803e-4dad-b9b4-8e7d706e574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.name for t in agent_state.tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801d036-cac7-4089-ab98-a164e858c285",
   "metadata": {},
   "outputs": [],
   "source": [
    " Memory blocks are returned as an unordered list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3147398-2aa1-4bb3-85c8-fa6ea8d7f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98621-b515-49a5-9d2c-fccb41c86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in client.agents.messages.list(agent_id=agent_state.id):\n",
    "    print_message(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4af94-7c59-4875-ad6b-a26e7eda36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = client.agents.passages.list(\n",
    "    agent_id=agent_state.id,\n",
    ")\n",
    "passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b30bd-4151-4b4c-a4ed-8d7d59d4d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Understanding core memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feac5b5-0256-47f4-ac13-1d212fbf1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Giving agent new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb56b72-4717-4105-8ea3-a9059d31d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a message to the agent\n",
    "response = client.agents.messages.create(\n",
    "    agent_id=agent_state.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"my name actually Sarah \"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# if we want to print the messages\n",
    "for message in response.messages:\n",
    "    print_message(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1352d7-286d-4dcb-a000-b6c7b1b11403",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.usage.completion_tokens)\n",
    "print(response.usage.prompt_tokens)\n",
    "print(response.usage.step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521d5a1-8f78-4ba4-b6dc-0b75e12ceb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieving new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526ca6d-c5fd-43e2-9d92-c742ce0683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.blocks.retrieve(\n",
    "    agent_id=agent_state.id,\n",
    "    block_label=\"human\"\n",
    ").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20d6db-22d5-4d72-a8b5-29dbca7e3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Understanding archival memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0cb1f7-8179-43a0-8dc6-08b5cfd5569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving information to archival memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c4b76-c797-44b5-b6fa-be458555597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = client.agents.passages.list(\n",
    "    agent_id=agent_state.id,\n",
    ")\n",
    "passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399170ee-cdc3-4c30-9617-3fb6090cde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.agents.messages.create(\n",
    "    agent_id=agent_state.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Save the information that 'bob loves cats' to archival\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# if we want to print the messages\n",
    "for message in response.messages:\n",
    "    print_message(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e3140-cff8-44ad-a842-055e2263661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = client.agents.passages.list(\n",
    "    agent_id=agent_state.id,\n",
    ")\n",
    "[passage.text for passage in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca67db-0163-4040-932b-a448d7ee5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explicitly creating archival memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af26fde-17a1-4ad9-aa2b-d5dc303cc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.passages.create(\n",
    "    agent_id=agent_state.id,\n",
    "    text=\"Bob's loves boston terriers\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcf34b-15a4-4f58-bbfd-aa8aeb21927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a message to the agent\n",
    "response = client.agents.messages.create(\n",
    "    agent_id=agent_state.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What animals do I like? Search archival.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for message in response.messages:\n",
    "    print_message(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6ebff-18d3-4ed4-a0ca-a22fce558826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a1374-dc79-4353-ada2-fd748b37be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_message(message):  \n",
    "    if message.message_type == \"reasoning_message\": \n",
    "        print(\"üß† Reasoning: \" + message.reasoning) \n",
    "    elif message.message_type == \"assistant_message\": \n",
    "        print(\"ü§ñ Agent: \" + message.content) \n",
    "    elif message.message_type == \"tool_call_message\": \n",
    "        print(\"üîß Tool Call: \" + message.tool_call.name + \"\\n\" + message.tool_call.arguments)\n",
    "    elif message.message_type == \"tool_return_message\": \n",
    "        print(\"üîß Tool Return: \" + message.tool_return)\n",
    "    elif message.message_type == \"user_message\": \n",
    "        print(\"üë§ User Message: \" + message.content)\n",
    "    elif message.message_type == \"usage_statistics\": \n",
    "        # for streaming specifically, we send the final chunk that contains the usage statistics \n",
    "        print(f\"Usage: [{message}]\")\n",
    "    else: \n",
    "        print(message)\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d9e9-3658-41b5-a38d-288adf318364",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Memory Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a7da4-bd21-435f-bbfc-aa58c70cac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951eede-442c-4a93-b7f9-4cb6bd087b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = client.agents.create(\n",
    "    memory_blocks=[\n",
    "        {\n",
    "          \"label\": \"human\",\n",
    "          \"value\": \"The human's name is Bob the Builder.\"\n",
    "        },\n",
    "        {\n",
    "          \"label\": \"persona\",\n",
    "          \"value\": \"My name is Sam, the all-knowing sentient AI.\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    embedding=\"openai/text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba89a7-aab2-44b5-9414-64254c125162",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accessing blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b5fa9-4ab2-48b3-a8dd-218cb44fec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = client.agents.blocks.list(\n",
    "    agent_id=agent_state.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f6396-dabc-49cb-96a8-abf98b16a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory blocks are returned as an unordered list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699550c-b5f6-4fae-8a39-47c8d296df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Replace the block_id with the id from the cell above.\n",
    "block_id='add_block_id_above'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed03f4d-cb46-46ba-94da-932bbaa6102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.blocks.retrieve(block_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d5dec-cc1f-4ec1-8be3-4fc1d3ca383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_block = client.agents.blocks.retrieve(\n",
    "    agent_id=agent_state.id,\n",
    "    block_label=\"human\",\n",
    ")\n",
    "human_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1f661-6873-495e-82ad-7801a1ae60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accessing block prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447c4b6-4d94-4bd1-9f4e-b050095daca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.core_memory.retrieve(\n",
    "    agent_id=agent_state.id\n",
    ").prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b844d1a-71d1-4e5b-a713-dd30b10a9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Accessing `AgentState` with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb447bf-8f60-4f31-9d06-9c84f389319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d06dc-79ef-4077-9e72-c4709ae1597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_id(agent_state: \"AgentState\"):\n",
    "    \"\"\"\n",
    "    Query your agent ID field\n",
    "    \"\"\"\n",
    "    return agent_state.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745abf27-d62c-4075-ab34-9bc52f6c9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_id_tool = client.tools.upsert_from_function(func=get_agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c26823-39d8-4861-8075-f7635929230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating agents that use tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef363e9-57f7-4814-aeb6-cd69e0f837f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = client.agents.create(\n",
    "    memory_blocks=[],\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    embedding=\"openai/text-embedding-3-small\",\n",
    "    tool_ids=[get_id_tool.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48626203-7fa4-4399-9137-baa54f82c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = client.agents.messages.create_stream(\n",
    "    agent_id=agent_state.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is your agent id?\" \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response_stream:\n",
    "    print_message(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263f7b4-2e37-46c2-8a34-a4b44eea5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Custom Task Queue Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d6ce8-271d-4dbb-9b10-a5105201ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating custom memory management tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db9985-8128-4505-84f4-35407e2f7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_queue_push(agent_state: \"AgentState\", task_description: str):\n",
    "    \"\"\"\n",
    "    Push to a task queue stored in core memory.\n",
    "\n",
    "    Args:\n",
    "        task_description (str): A description of the next task you must accomplish.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: None is always returned as this function\n",
    "        does not produce a response.\n",
    "    \"\"\"\n",
    "\n",
    "    from letta_client import Letta\n",
    "    import json\n",
    "\n",
    "    client = Letta(base_url=\"http://localhost:8283\")\n",
    "\n",
    "    block = client.agents.blocks.retrieve(\n",
    "        agent_id=agent_state.id,\n",
    "        block_label=\"tasks\",\n",
    "    )\n",
    "    tasks = json.loads(block.value)\n",
    "    tasks.append(task_description)\n",
    "\n",
    "    # update the block value\n",
    "    client.agents.blocks.modify(\n",
    "        agent_id=agent_state.id,\n",
    "        value=json.dumps(tasks),\n",
    "        block_label=\"tasks\"\n",
    "    )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba7034-d4db-418e-990f-795e38c83e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_queue_pop(agent_state: \"AgentState\"):\n",
    "    \"\"\"\n",
    "    Get the next task from the task queue \n",
    " \n",
    "    Returns:\n",
    "        Optional[str]: Remaining tasks in the queue\n",
    "    \"\"\"\n",
    "\n",
    "    from letta_client import Letta\n",
    "    import json \n",
    "\n",
    "    client = Letta(base_url=\"http://localhost:8283\") \n",
    "\n",
    "    # get the block \n",
    "    block = client.agents.blocks.retrieve(\n",
    "        agent_id=agent_state.id,\n",
    "        block_label=\"tasks\",\n",
    "    )\n",
    "    tasks = json.loads(block.value) \n",
    "    if len(tasks) == 0: \n",
    "        return None\n",
    "    task = tasks[0]\n",
    "\n",
    "    # update the block value \n",
    "    remaining_tasks = json.dumps(tasks[1:])\n",
    "    client.agents.blocks.modify(\n",
    "        agent_id=agent_state.id,\n",
    "        value=remaining_tasks,\n",
    "        block_label=\"tasks\"\n",
    "    )\n",
    "    return f\"Remaining tasks {remaining_tasks}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dcf4b-29ef-4432-ae6e-66879449a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upserting tools into Letta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0585-49e3-4f7b-9776-ef00b67f8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_queue_pop_tool = client.tools.upsert_from_function(\n",
    "    func=task_queue_pop\n",
    ")\n",
    "task_queue_push_tool = client.tools.upsert_from_function(\n",
    "    func=task_queue_push\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c252a1-8bc3-475c-82de-ec70ad78185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "task_agent = client.agents.create(\n",
    "    system=open(\"task_queue_system_prompt.txt\", \"r\").read(),\n",
    "    memory_blocks=[\n",
    "        {\n",
    "          \"label\": \"tasks\",\n",
    "          \"value\": json.dumps([])\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    embedding=\"openai/text-embedding-3-small\", \n",
    "    tool_ids=[task_queue_pop_tool.id, task_queue_push_tool.id], \n",
    "    include_base_tools=False, \n",
    "    tools=[\"send_message\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b985bd0-6030-4f3e-843f-825fb7dd561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tool.name for tool in task_agent.tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627174-0a69-4826-9d14-5468919ee81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.blocks.retrieve(task_agent.id, block_label=\"tasks\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e2746-2310-4335-8388-97872da8c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using task agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1eb51-baf9-45b9-9570-71babf21d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = client.agents.messages.create_stream(\n",
    "    agent_id=task_agent.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Add 'start calling me Charles' and \"\n",
    "            + \"'tell me a haiku about my name' as two seperate tasks.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response_stream:\n",
    "    print_message(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6f421-a14b-4b3a-8569-32172e3c0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = client.agents.messages.create_stream(\n",
    "    agent_id=task_agent.id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Complete your tasks\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response_stream:\n",
    "    print_message(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e587e-51cb-4c53-b88d-b906fd6f6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieving task list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5a27a-902a-4e0f-828f-6bad758da904",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.blocks.retrieve(block_label=\"tasks\", agent_id=task_agent.id).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091003d5-4dff-41e6-911d-5eadf8d4b372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55746330-200f-4e0b-8786-a457c4aa6342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
